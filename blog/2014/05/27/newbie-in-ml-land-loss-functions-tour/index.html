
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>How not to be lost in the world of loss functions - Random thoughts</title>
  <meta name="author" content="Duong Nguyen">

  
  <meta name="description" content="It has been a while since my last post in series of machine learning for newbie. This time, we will discuss a bit on the loss functions. But what is &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://blog4fun.github.io/blog/2014/05/27/newbie-in-ml-land-loss-functions-tour">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Random thoughts" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
        displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
      //,
      //displayAlign: "left",
      //displayIndent: "2em"
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-51112211-1']);
    _gaq.push(['_setDomainName', 'github.io']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Random thoughts</a></h1>
  
    <h2>Share to Learn, Learn to Share</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:blog4fun.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">How Not to Be Lost in the World of Loss Functions</h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-05-27T14:46:00+09:00" pubdate data-updated="true">May 27<span>th</span>, 2014</time>
        
      </p>
    
  </header>


<div class="entry-content"><p>It has been a while since my last post in series of <strong>machine learning for newbie</strong>. This time, we will discuss a bit on <strong>the loss functions</strong>. But what is the loss function, anyway?     </p>

<h4 id="preliminary">0. Preliminary</h4>
<p>Let&#8217;s get started with the problem of making prediction on new unseen data based on given training data. This problem includes both regression and classification as its specific cases. More specificially, we consider a predictive function $f: \mathcal{X} \rightarrow \mathcal{Y}$, with input (covariate, feature vector) $x \in \mathcal{X}$, and output (label) $y \in \mathcal{Y}$. For a given $x$, we try to predict its corresponding output as $f(x)$ such that $f(x)$ is as close to the true output as possible. It&#8217;s clear that we need some ways to measure how close (how good) our prediction is to the true value. That where a loss function comes in. In this post, unless stated otherwise, we consider a loss function $l: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathcal{R}$ such that $l(f(x), y)$ tell you how close our prediction $f(x)$ to the true value $y$ in a quantitative manner. The problem of making prediction is simply equivalent to learning the predictive function <code>f(x)</code> which minimizes the expected risk:</p>

<p><script type="math/tex">~~~~~~~~~~~~~~~~~~~\underset{f \in \mathcal{F}}{\mathrm{min}}\:\mathbf{E}_{P(x,y)}[l(f(x),y)]</script>  (*)</p>

<p>where $\mathcal{F}$ is our model from which we select the predictive function, P(x,y) is the underlying joint distribution of input <code>x</code> and output <code>y</code>. We will talk about model selection and how to minimize the expected risk above, e.g. empirical risk minimization (ERM) in other posts. For now, let us focus on the problem of choosing the appropriate loss function. More specifically, different types of loss functions might lead to different solutions of the minimization problem above. In other words, the loss function should be carefully selected depending on specific problems, and our purposes. From now, we will take a tour over some common loss functions and their usage.</p>

<h4 id="i-loss-functions-in-regression">I. Loss Functions in Regression</h4>
<p>For simplicity, we often consider $\mathcal{Y} = \mathbf{R}$ in regression setting.</p>

<h5 id="squared-loss">1. Squared Loss:</h5>
<p><script type="math/tex">~~~~~~~~~~~l(f(x),y) = (f(x) - y)^2</script>.  <br />
This is one of the most natural loss function we might come up with at the first place. Under this loss function, our expected solution to the minimization problem (*) is: <script type="math/tex">f(x) = \mathbf{E}[Y|X=x]</script> for a given input <script type="math/tex">x</script>. <br />
In other words, we will obtain the <strong>conditional mean</strong> as the solution of ordinary least squares regression with the <strong>squared loss</strong> function. </p>

<h5 id="tau-quantile-loss">2. $\tau$-quantile loss:</h5>
<p>$~~~~~~~~~~l(f(x),y) = (1-\tau)\mathrm{max}(f(x)-y,0) + \tau\mathrm{max}(y-f(x),0)$,  <br />
for some $\tau \in (0,1)$.  <br />
Under this loss function, our expected solution for (*) is: <script type="math/tex">% <![CDATA[
\{y~|~\mathrm{Pr}(Y < y) \leq \tau \leq \mathrm{Pr}(Y \leq y)\} %]]></script>. In other words, we obtain the $\tau$-quantile point of the conditional probability $P(Y|X=x)$. Especially, when $\tau = 0.5$, $l(f(x), y) = |f(x)-y|$, and the solution is the median of the conditional probability $P(Y|X=x)$.</p>

<h4 id="ii-loss-functions-in-classification">II. Loss functions in Classification</h4>
<p>Here we only consider the binary classification problem where <script type="math/tex">\mathcal{Y} = \{+1, -1\}</script>. However, the loss functions discussed here also generalize to the multiclass problems as well.     </p>

<h5 id="loss">1. 0-1 loss:</h5>
<p>In binary classification, our objective is to minimize the misclassfication error as    <br />
<script type="math/tex">~~~~~~~~~~~~~~~~~~~~~~\underset{f \in \mathcal{F}}{\mathrm{min}}~E\{\boldsymbol{1}[y \neq f(x)]\}</script> (**)      <br />
where <script type="math/tex">f(x) \in \{+1, -1\}</script>, and $l(f(x), y) = \boldsymbol{1}[y \neq f(x)]$ is called 0-1 loss, such that:   </p>

<script type="math/tex; mode=display">% <![CDATA[
\boldsymbol{1}[y \neq f(x)] = \left\{\begin{matrix}
1 & \mathrm{if} & y \neq f(x)\\ 
0 &  & \mathrm{otherwise} 
\end{matrix}\right. %]]></script>

<p>The predictive function that minimizes the objective <code>(**)</code> above is called the optimal Bayes classifier:     </p>

<script type="math/tex; mode=display">% <![CDATA[
f^{*}(x) = \left\{\begin{matrix}
+1 & \mathrm{if} & P(y=1|x) > P(y=-1|x)\\ 
-1 & \mathrm{if} & P(y=1|x) < P(y=-1|x)\\ 
(-1~\mathrm{or}~+1) &  & \mathrm{otherwise} 
\end{matrix}\right. %]]></script>

<p>Astute readers might notice that the discreteness of output domain ${+1, -1}$ makes our predictive function not quite comfortable to handle. So, let us consider a predictive function similar to regression case, i.e. $f: \mathcal{X} \rightarrow \mathbf{R}$, and just take the sign of this function to classify the input,    </p>

<script type="math/tex; mode=display">% <![CDATA[
 \mathrm{sign}(f(x)) = \left\{\begin{matrix}
+1 & \mathrm{if} & f(x) > 0\\ 
-1 & \mathrm{if} & f(x) < 0\\ 
(-1~\mathrm{or}~+1) &  & \mathrm{otherwise} 
\end{matrix}\right. %]]></script>

<p>This further leads to the concept, called <code>margin</code>, defined as $m(x) = yf(x)$. Intuitively, a positive/negative margin means correct/wrong classification. Intuitively, the larger than 0 the margin is, the more confidence we have in our prediction. This idea is related to maximum margin classifier, where the popular <strong>Support Vector Machine (SVM)</strong> is an example.  <br />
Another problem is that while there is nothing wrong with the straightforward 0-1 loss, its non-convexity leads to difficultly in solving our optimization problem. In general, there are no guarantees about minimization with non-convex optimization functions. That&#8217;s why it is common practice in machine learning that people uses convex loss functions, whose convexities guarantees that we can find the (unique) minimum for our optimization problem, as a proxy for the 0-1 loss. See, e.g., <a href="http://www.eecs.berkeley.edu/~wainwrig/stat241b/bartlettetal.pdf">Peter L. Bartlett, et al. Convexity, Clasification, and Risk Bounds</a> for theoretical justification of using the convex surrogate losses. Now, let&#8217;s take a look at some common convex surrogate losses in classfication.</p>

<h5 id="convex-surrogate-losses">2. Convex surrogate losses</h5>
<p>The convex surrogate losses are often defined as functions of the <code>margin</code> mentioned above. Intuitively, the surrogate loss should be a monotonically decreasing function, i.e., the larger the margin, the smaller the incurred loss shoud be.        </p>

<h6 id="logistic-loss">* Logistic loss</h6>
<p><script type="math/tex">~~~~~~~~~~l(f(x), y) = log(1 + exp(-yf(x))) = log(1 + exp(-m(x)))</script>.   <br />
This loss function is used in <strong>Logistic regression</strong>, which actually is a classification technique despite its name. From the probabilistical perspective of logistic regression, minimizing the logistic loss is equivalent to maximizing the conditional log-likelihood <code>log P(Y|X)</code>.</p>

<h6 id="hinge-loss">* Hinge loss</h6>
<p><script type="math/tex">~~~~~~~~~~l(f(x), y) = max(1-yf(x), 0) = max(1-m(x),0)</script>.    <br />
This loss function is used in <strong>Support Vector Machine</strong>. Since it is unbounded as the margin is getting negatively smaller, this loss function may be sensitive to outlier in the data. Its bounded variant, called <strong>Ramp loss</strong> is proposed to be more robust against outlier.   <br />
<script type="math/tex">~~~~~~~~~~l_{Ramp}(f(x), y) = min(1, max(1-yf(x),0))</script>.    </p>

<h6 id="exponential-loss">* Exponential loss</h6>
<p><script type="math/tex">~~~~~~~~~~l(f(x), y) = exp(-yf(x)) = exp(-m(x))</script>.  <br />
This loss function is often used in <strong>Boosting algorithm</strong>, such as <code>Adaboost</code>. What <code>Adaboost</code> actually does is minimizing the exponential loss function with training data. Again, its sensitiveness to noise in data leads to many proposed robust boosting algorithms, e.g., <code>Madaboost</code>, <code>Logitboost</code> which use more robust loss functions.    </p>

<script type="math/tex; mode=display">% <![CDATA[
l_{mada}(f(x), y) = \left\{\begin{matrix}
-m(x) + 1/2 & \mathrm{if} & m(x) <= 0\\ 
exp(-2m(x))/2 & \mathrm{if} & m > 0 
\end{matrix}\right. %]]></script>

<p><script type="math/tex">~~~~~~~~~~l_{logit}(f(x), y) = log(1+exp(-2m(x)))</script>.</p>

<p>Since a picture might worth a thousand words, let me conclude with an illustrative figure, including plots of loss functions we discussed in this long post. Until next time!
<img src="/images/loss.png" /></p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Duong Nguyen</span></span>

      








  


<time datetime="2014-05-27T14:46:00+09:00" pubdate data-updated="true">May 27<span>th</span>, 2014</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/classification/'>classification,</a>, <a class='category' href='/blog/categories/function/'>function,</a>, <a class='category' href='/blog/categories/loss/'>loss</a>, <a class='category' href='/blog/categories/regression/'>regression</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://blog4fun.github.io/blog/2014/05/27/newbie-in-ml-land-loss-functions-tour/" data-via="" data-counturl="http://blog4fun.github.io/blog/2014/05/27/newbie-in-ml-land-loss-functions-tour/" >Tweet</a>
  
  
  <div class="g-plusone" data-size="medium"></div>
  
  
    <div class="fb-like" data-send="true" data-width="450" data-show-faces="false"></div>
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2014/04/13/simple-combinatorics-cheetsheat/" title="Previous Post: Simple Combinatorics Cheetsheat">&laquo; Simple Combinatorics Cheetsheat</a>
      
      
    </p>
  </footer>
</article>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/05/27/newbie-in-ml-land-loss-functions-tour/">How Not to Be Lost in the World of Loss Functions</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/04/13/simple-combinatorics-cheetsheat/">Simple Combinatorics Cheetsheat</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/02/25/functional-programming-in-c-plus-plus/">Functional Programming in C++</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/02/12/c-plus-plus-preprocessor-explained/">C++ Preprocessor Explained</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/01/29/hash-based-join-algorithms-for-rdbms/">Hash-based Join Algorithms</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating...</li>
  </ul>
  
  <a href="https://github.com/ntduong">@ntduong</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'ntduong',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - Duong Nguyen -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id; js.async = true;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>



  <script type="text/javascript">
    (function() {
      var script = document.createElement('script'); script.type = 'text/javascript'; script.async = true;
      script.src = 'https://apis.google.com/js/plusone.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(script, s);
    })();
  </script>



  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
