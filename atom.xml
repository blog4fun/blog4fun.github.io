<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[A little ninja blog]]></title>
  <link href="http://blog4fun.github.io/atom.xml" rel="self"/>
  <link href="http://blog4fun.github.io/"/>
  <updated>2013-12-20T19:18:54+09:00</updated>
  <id>http://blog4fun.github.io/</id>
  <author>
    <name><![CDATA[Duong Nguyen]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Regression and learning models]]></title>
    <link href="http://blog4fun.github.io/blog/2013/12/20/regression-and-learning-models/"/>
    <updated>2013-12-20T16:01:00+09:00</updated>
    <id>http://blog4fun.github.io/blog/2013/12/20/regression-and-learning-models</id>
    <content type="html"><![CDATA[<p>In this series of posts, I will <strong>informally</strong> discuss some basic things in machine learning theory which I&#8217;ve learnt through my own research experience, lectures, etc. Feel free to leave your comments and share your thoughts in the comment section below. </p>

<p>In this very first post, let&#8217;s get started with one of the most standard problems in <em>supervised learning</em> setting, called <em>regression</em> and some common <em>learning models</em>.</p>

<h3 id="regression-as-functional-approximation">Regression as functional approximation</h3>
<p>We consider the problem of estimating an <em>unknown</em> function <script type="math/tex">f(x)</script>, using a set of samples <script type="math/tex">\{(x_i, y_i)\}</script>, where
<script type="math/tex">y_i = f(x_i) + e_i</script>, and <script type="math/tex">e_i</script> is the <em>i.i.d</em> Gaussian noises. This is a common formulation of regression problem in standard supervised learning setup, where <script type="math/tex">\{(x_i, y_i)\}</script> is often mentioned as <em>training</em> samples, $x_i \in \mathcal{R}^d$ is d-dimensional input vector, and $y_i$ is its corresponding output. For simplicity, we consider only the case where $y_i$ is scalar. <br />
One way to solve the problem above is to search for the <strong>true</strong> function <script type="math/tex">f(x)</script> in a set of functions that is parameterized by parameter vector <script type="math/tex">\theta</script> (a.k.a., a parametric model indexed by parameter <script type="math/tex">\theta</script>). Obviously, if we don&#8217;t have any prior knowledge (about the form of $f(x)$) other than training samples, it&#8217;s almost impossible to obtain exact true function $f(x)$. Instead, we try to find a function in a given model which best approximates $f(x)$. That&#8217;s why we can see regression as functional approximation problem. We often &#8220;learn&#8221; the parameter $\theta$ from training samples by casting our problem into an optimization problem, e.g., minimizing the approximation error. An estimation of $f(x)$, denoted $\hat{f}(x)$ can be obtained by substituting optimized parameter into the model formulation. I will return to this point in later posts. <br />
Below, we discuss some commonly used <strong>parametric models</strong>, with general form
<script type="math/tex">\{f(x; \boldsymbol{\theta})~|~ \boldsymbol{\theta} = (\theta_1,...,\theta_b)^\top\}</script>.</p>

<h3 id="learning-model">Learning model</h3>
<p><strong>1. Linear model</strong>   <br />
Instead of a <em>linear-in-input-feature</em> model, which is often introduced in some stats/ML introductory books/courses, we consider a more general <strong>linear-in-parameter</strong> model:   </p>

<p><script type="math/tex">\:\:\:\:\:\:\:\:f(x; \boldsymbol{\theta}) = \sum_{j=1}^b\theta_j\psi_j(x)</script>, where <script type="math/tex">x \in \mathcal{R}^d</script>.  </p>

<p>This <em>linear-in-parameter</em> model includes the former as a special case. We might think this type of model is quite limited due to its linearity, but actually it&#8217;s quite flexible. Particularly, we can customize the basis functions ${\psi_j(x)}$ as freely as we want based on specific problems. For examples, polynomial basis functions or trigonometric basis functions are common choices when d = 1. For high dimensional case, some powerful linear models can be used:  <br />
+ Multiplicative model: It can model very complicated functions in high dimension, but due to very large number of parameters (exponential order w.r.t to the dimensionality d), it can only be applied to <em>not-so-high</em> dimensional case.</p>

<p><script type="math/tex">f(x; \boldsymbol{\theta}) = \sum_{j_1}...\sum_{j_d}\theta_{j_1,...,j_d}\psi_{j_1}(x^{(1)})...\psi_{j_d}(x^{(d)})</script>.</p>

<ul>
  <li>Additive model: much simpler with smaller number of parameters (linear order w.r.t to the dimensionality d) than multiplicative model. Obviously, its expressiveness is more limited than multiplicative model. </li>
</ul>

<p><script type="math/tex">\:\:\:\:\:\:\:\:\:\:f(x; \boldsymbol{\theta}) = \sum_{k=1}^d\sum_j \theta_{k,j}\psi_j(x^{(k)})</script>.</p>

<p><strong>2. Kernel model</strong> </p>

<p><script type="math/tex">\:\:\:\:\:\:\:\:\:\:f(x; \boldsymbol{\theta}) = \sum_{j=1}^n \theta_{j} K(x, x_j)</script>.</p>

<p>It is linear-in-parameter but unlike the linear model discussed above, its basis functions depend on training samples <script type="math/tex">\{x_j\}</script>.    <br />
+ The number of parameters is generally independent of the dimensionality d of input. <br />
+ It can be seen as a linear-in-parameter model.  <br />
+ It depends on the training samples, and thus its properties is a bit different from ordinary linear model. That&#8217;s why it&#8217;s also known as <strong>non-parametric</strong> model. Discussion on non-parametric model is beyond the scope of this post. Feel free to refer to [books] on this topic. In this post, we do not go into the detailed and complicated analysis, then unless otherwise stated, we consider kernel model as a specific case of linear model. <br />
+ It can capture and incoporate characteristics of training samples. This might be useful, but on the other hand, it might be sensitive to the noisy training samples.</p>

<p><strong>3. Non-linear model</strong>    <br />
Simply put, every non-linear w.r.t parameters is called <strong>non-linear</strong> model. For examples, hierachical model (a multi-layer model in perceptron, neuron network, etc.) is well-known, given the popularity of <strong>deep learning</strong>.</p>

<p><strong>To be continuted and updated later!</strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On sparse matrix storage schemes (Novice-level)]]></title>
    <link href="http://blog4fun.github.io/blog/2013/11/29/memo-on-sparse-matrix-storage-schemes-novice-level/"/>
    <updated>2013-11-29T17:00:00+09:00</updated>
    <id>http://blog4fun.github.io/blog/2013/11/29/memo-on-sparse-matrix-storage-schemes-novice-level</id>
    <content type="html"><![CDATA[<p>In this post, I will discuss some simple storage schemas for sparse matrices. <br />
<strong>Disclaimer</strong>: This <em>informal</em> note is mainly based on my understanding so far, and for my learning purposes. For more accurate and detailed discussion, please refer to your reliable sources.</p>

<p>A sparse matrix is a matrix, which is almost all <em>zeros</em>. 
In general, a <em>zero</em> can be thought as a meaningless value that we don&#8217;t need to store.
In other words, storing only nonzero items for a sparse matrix is enough. As a result, we can save a lot of memory space, or time to transfer compressed data over networks, etc. But, keep in mind that there might be a <strong>trade-off</strong> between memory efficiency and fast/easy access to nonzero items. We will discuss more about this problem in later posts (I hope so!).</p>

<p>Let M denote a sparse matrix of size <code>R x C</code>. Assume that M contains <code>n</code> nonzero items, where <code>n</code> is significantly smaller than <code>R x C</code>. Below are three simple yet useful storage schemes:<br />
<strong>1. Coordinate Format (COO)</strong>    <br />
A list of triplets <code>(r,c,val)</code>, where <code>val = M[r][c]</code>.   </p>

<p><strong>2. Compressed Sparse Row Format (CSR)</strong> <br />
Basically, all information is stored in 3 arrays:   <br />
  * <code>vals</code>: array of nonzero items in left-to-right, then top-to-bottom order. Hence, vals is a length <code>n</code> array. <br />
  * <code>col_ind</code>: array of column indices (0-based) of nonzero items. Hence, col_ind is a length <code>n</code> array. <br />
  * <code>rs_ind</code>: (index of first nonzero item of each row in vals array). Hence, rs_ind is a length <code>(R+1)</code> array. All nonzero items of M&#8217;s i-th row lies in range <code>[rs_ind[i], rs_ind[i+1]-1]</code> of vals array. That is, if we denote <code>le = rs_ind[i], ri = rs_ind[i+1]-1</code>, then <code>vals[le..ri]</code> are all nonzero items in i-th row of M.  </p>

<p><strong>3. Compressed Sparse Column Format (CSC)</strong>  <br />
This format can be thought as CSR with the roles of row and column exchanged. Then, I will skip the explanation here.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hello World]]></title>
    <link href="http://blog4fun.github.io/blog/2013/11/01/hello-world/"/>
    <updated>2013-11-01T01:40:00+09:00</updated>
    <id>http://blog4fun.github.io/blog/2013/11/01/hello-world</id>
    <content type="html"><![CDATA[<p>Today, I&#8217;m pretty excited to start blogging with a geeky, yet fun Octopress! <br />
As a newbie, I&#8217;ll jot down a basic setting of Octopress-based blogs in this very first post.</p>

<p><strong>1. Test sharing code snippets</strong>  </p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span> (test.py)</span> <a href="http://blog4fun.github.io/downloads/code/test.py">download</a></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">insert_code</span><span class="p">():</span>
</span><span class="line">	<span class="k">print</span> <span class="s">&quot;Sorry, it does not work!&quot;</span>
</span><span class="line">	
</span><span class="line"><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&quot;__main__&quot;</span><span class="p">:</span>
</span><span class="line">	<span class="n">insert_code</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>Check out <a href="http://octopress.org/docs/plugins/codeblock/">this guide</a>.<br />
<strong>UPDATE (2013/12/20):</strong> Having switched to kramdown instead of default rdiscount, all of my backtick, codeblock, etc. no longer work. For the moment, I don&#8217;t know why, so let&#8217;s take time and figure it out&#8230; </p>

<p><strong>2. Markdown</strong> <br />
A simple markdown <a href="https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet">cheat-sheet</a>, and an online markdown <a href="http://www.ctrlshift.net/project/markdowneditor/">editor</a></p>
]]></content>
  </entry>
  
</feed>
